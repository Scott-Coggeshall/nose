---
title: "NOSE study"
author: "Scott Coggeshall"
date: "6/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)
library(kableExtra)
library(rstan)
```


```{r}


p_table <- function(p_disease, test_sens, test_spec){
  
  p_00 <- prod(p_disease, 1 - test_sens) + prod(1 - p_disease, test_spec)
  
  p_11 <- prod(p_disease, test_sens) + prod(1 - p_disease, 1 - test_spec)
  
  p_01 <- p_disease*test_sens[2]*(1 - test_sens[1]) + (1 - p_disease)*test_spec[1]*(1 - test_spec[2])
  
  p_10 <- 1 - (p_00 + p_11 + p_01)
  
  c(p_00, p_01, p_10, p_11)
  
}


```


Let $T_1$ and $T_2$ stand for the binary results of the SoC and Alt swab, respectively, where 1 indicates presence of disease $D$ and 0 indicates absence of disease.

The basic unit of observed data at the site level is a two-by-two table of the cross-classified results of the two tests. This can be parameterized in terms of the 4 probabilities:

$$
\begin{align*}
    p_{00} &= P(T_1 = 0, T_2 = 0) \\
    p_{01} &= P(T_1 = 0, T_2 = 1) \\
    p_{10} &= P(T_1 = 1, T_2 = 0) \\
    p_{11} &= P(T_1 = 1, T_2 = 1)
\end{align*}
$$

Quantities of interest regarding the agreement between the two tests can be computed from this table, i.e. overall agreement can be calculated as 

$$
P(Agree) = p_{00} + p_{11}.
$$

## Direct Empirical Approach 

One possible approach is to ignore any consideration of the underlying characteristics of the test and just work directly with these observed quantities. For instance, we can model the observed agreement between the tests at a given site as

$$
N_{Agree} \sim Binomial(n, P(Agree))
$$

and then base power/sample size calculations on the hypothesis test

$$
H_0: P(Agree) < .95 \qquad H_1: P(Agree) \geq .95
$$

Power curves to detect agreement above $95\%$ at different samples sizes are shown below

```{r}

binom_power <- function(p, n, alpha, alternatives, lower_tail){
  
  cutoffs <- qbinom(p = alpha, size = n, prob = p, lower.tail = lower_tail)
  
  
  n_mat <- matrix(n, nrow = length(n), ncol = length(alternatives))
  
  cutoffs_mat <- matrix(cutoffs, nrow = length(cutoffs), ncol = length(alternatives))
  
  alternatives_mat <- matrix(alternatives, nrow = length(n), ncol = length(alternatives), byrow = T)
  
  pbinom(q = cutoffs_mat, size = n_mat, prob = alternatives_mat, lower.tail = lower_tail)
  
  
  
  
}


test <- binom_power(.95, n = seq(from = 200, to = 2000, by = 10), alpha = .05, alternatives = seq(from = .95, to = 1, by = .001), lower_tail = F)


plot(seq(from = .95, to = 1, by = .001), test[1, ], type = 'l', xlab = "P(Agree)", ylab = "Power")

alts <- seq(from = .95, to = 1, by = .001)

lines(alts, test[10, ], col = 'blue')


lines(alts, test[(nrow(test) - 1)/2, ], col = "red")

lines(alts, test[nrow(test), ], col = 'green')

abline(h = .8, lty = 2 , col = 'red')

legend("bottomright", legend = paste("n =", c(200, 500, 1000, 2000)), col = c("black", "blue", "red", "green"), lty = 1)


ae_power <- binom_power(.05, n = seq(from = 200, to =2000, by = 10), alpha = .05, alternatives = seq(from = 0, to = .05, by = .001), lower_tail = T)

alt_seq <- seq(from = 0, to = .05, by = .001)


```

Planned interim analyses for safety and/or inferiority can be incorporated into this testing procedure.

We can consider either an overall marginal probability of agreement or we can use mixed-effects models to explicitly model site-specific agreement probabilities.



## Incorporating Information/Uncertainty about Underlying Prevalence and Test Performance


Another approach is to work from a "generative" model in which we model the identified probabilities that generate the observed 2x2 table in terms of the (unidentified) underlying disease prevalence and true sensitivity/specificities of the SoC and Alt swabs.

Let $p_D$ stand for underlying disease prevalence among the population being tested and let $Sens_{t}$ and $Spec_{t}$ stand for the *true* sensitivity and specificity of test $T_t, t = 0, 1$. For example, $Sens_1 = P(Test_1 = 1 \mid D = 1)$ We can express the cell probabilities $p_{00}, p_{01}, p_{10}, p_{11}$ in terms of these underlying disease prevalence and test characteristics. We show the full derivation for $p_{00} = P(Test_1 = 0, Test_2 = 0)$ and then state the results for the remaining cell probabilities since they are derived in a similar manner.

$$
\begin{align*}
    P(Test_1 = 0, Test_2 = 0) &= P(Test_1 = 0, Test_2 = 0 \mid D = 1)P(D = 1) \\
    &\qquad+ P(Test_1 = 0, Test_2 = 0 \mid D = 0)P(D = 0) \\
    &= P(Test_1 = 0 \mid D = 1)P(Test_2 = 0 \mid Test_1 = 0, D = 1)P(D = 1)\\
    &\qquad + P(Test_1 = 0 \mid D = 0) P(Test_2 = 0 \mid Test_1 = 0, D = 0)P(D = 0) \\
    &= (1 - Sens_1)\times(1 - Sens_2)\times p_D + Spec_1 \times Spec_2 \times (1 - p_D).
\end{align*}
$$

The full set of equations relating the cell probabilities $p_{00}, p_{01}, p_{10}, p_{11}$ to underlying disease prevalence and test characteristics is given by 
$$
\begin{align*}
    p_{00} &= (1 - Sens_1) \times (1 - Sens_2) \times p_D + Spec_1 \times Spec_2 \times (1 - p_D) \\
    p_{01} &= (1 - Sens_1)\times Sens_2 \times p_D +  Spec_2 \times (1 - Spec_1) \times (1 - p_D)\\
    p_{11} &= Sens_1 \times Sens_2 \times p_D + (1 - Spec_1) \times (1 - Spec_2) \times (1 - p_D)
\end{align*}
$$

We leave off the final probability $p_{10}$ since it is determined by the other three through the requirement that $p_{00} + p_{01} + p_{10} + p_{11} = 1.$

Under an assumption of *conditional independence*

$$
\begin{align*}
    P(T_2 \mid T_1, D) &= P(T_2 \mid D)
\end{align*}
$$

then $Sens_2$ and $Spec_2$ correspond to the unconditional sensitivity and specificity of the second test. In the presence of an order effect, we would not expect the conditional independence assumption to hold. In that case, $Sens_2$ and $Spec_2$ correspond to a conditional sensitivity and specificity.


The key to using this approach is to work within a Bayesian framework where we specify our prior knowledge about the underlying parameters in terms of probability distributions, and then use the data to update that knowledge.

## Prior Specification

We can place prior distributions on the underlying disease prevalence and test characteristics and thereby elicit priors for the cell probabilities $p_{00}, \ldots, p_{11}$. The priors for disease prevalence and test characteristics can in turn be informed by our underlying knowledge of the disease and the testing procedure. As an example, let's consider $Spec_1$, the specificity of the SoC swab. From the underlying science of RT-PCR, we expect false positives (i.e. the supposed detection of viral matter when in fact none is present in the test subject) to be relatively rare. Equivalently, we expect *a priori* that specificity of the SoC swab will be high, and we would be very surprised if the specificity of the SoC was below say .7. We can represent this prior information as a probability distribution over the interval $[0, 1]$. 

```{r}

curve(dbeta(x, 2.5, 1), xlab = "Specificity of SoC", ylab = "Prior")

```

This distribution encodes our prior belief, based on our understanding of the science behind the SoC swab, that higher values for the specificity of the SoC swab are more plausible than lower. Importantly, however, this prior doesn't preclude the possibility that the specificity of the SoC swab is low.

Priors for the other parameters can be formed in a similar way by considering the range of values we consider to be plausible. If we really don't have any prior knowledge about one of the parameters, we can specify a "flat" prior in which all values are considered equally probable.

**Likelihood**

We then need to specify a likelihood, which is just a model for the process by which the observed data arise as a function of the unobserved parameters. 

Let the observed data be represented by $\boldsymbol{X} = (X_{00}, X_{01}, X_{10}, X_{11})$ where for $t_1 = 0, 1$ and $t_2 = 0, 1$ we have

$$
\begin{align*}
    X_{t_1 t_2} &= \text{Number of people with $T_1 = t_1$ and $T_2 = t_2$}.
\end{align*}
$$
We can model these data using a multinomial likelihood



$$
\begin{align*}
    L(\boldsymbol{X} \mid \boldsymbol{\theta}) &= \frac{n!}{x_{00}! x_{01}! x_{10}! x_{11}!} p_{00}^{x_{00}} p_{01}^{x_{01}} p_{10}^{x_{10}} p_{11}^{x_{11}}
\end{align*}
$$

This is the exact same likelihood or data-generating model that we would assume in the direct empirical approach considered above.

## Posterior Distribution

Inference in a Bayesian analysis is based on the *posterior distribution*, which can be thought as an update of the prior distribution based on the observed data. A major feature of inference based on a posterior distribution is that the updating can happen sequentially. As new data come in, the posterior distribution can be recalculated whenever we like to summarize our updated beliefs based on the data seen so far.

Formally, given a prior on the model parameters $p(\boldsymbol{\theta})$ and a likelihood for the data $f(y \mid \boldsymbol{\theta})$, the posterior distribution is calculated as

$$
\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}) = \frac{p(\boldsymbol{\theta})f(\boldsymbol{y} \mid \boldsymbol{\theta})}{f(\boldsymbol{y})}.
\end{align*}
$$
The characteristics of this distribution such as the central tendencies or various quantiles then form the basis for inference. The set of models for which the posterior distribution can be computed *analytically* is relatively limited. In practice, what we do instead is use procedures such as Markov Chain Monte Carlo or, more recently, Hamiltonian Monte Carlo to draw random samples from the posterior distribution. Inferential quantities such as a posterior mean or a 95\% uncertainty interval are then calculated based on the draws from the posterior distribution.

## Simulated Examples 
As an example, we will simulate data from a swab experiment conducted at a single site, and then analyze the data using the Bayesian procedure described above using several priors with different degrees of informativeness. We will simulate data under the following assumptions about the generative model:

- underlying prevalence of disease $p_D$ is 0.20
- Sensitivity and specificity of the SoC swab are 0.95 and 0.99, respectively
- Sensitivity and specificity of the alternative swab are 0.95 and 0.99 as well

```{r}

p_table_example <- p_table(.2, c(.95, .95), c(.99, .99))

```
These values for the underlying generative model produce the following cell probabilities for the observed 2x2 table of test results

$(p_{00}, p_{01}, p_{10}, p_{11})$ = (`r round(p_table_example, digits = 2)`)

From this distribution over the cell probabilities we can calculate various agreement measures. For instance, overall agreement is `r round(sum(p_table_example[c(1, 4)]), digits = 2)`.

We'll start by fitting a model using noninformative flat priors over the disease prevalence and swab sensitivities and specificities.

Our priors for the model parameters are

$$
\begin{align*}
p_D &\sim Uniform(0, 1) \\
Sens_1 &\sim Uniform(0, 1) \\
Sens_2 &\sim Uniform(0, 1) \\
Spec_1 &\sim Uniform(0, 1) \\
Spec_2 &\sim Uniform(0, 1)
\end{align*}
$$
Together, these priors on disease prevalence and swab characteristics induce a prior over the cell probablities $(p_00, p_01, p_10, p_11)$. We visualize this prior by drawing 10000 samples from the prior distributions given above and then computing the corresponding cell probabilities. The results are shown below
```{r}
set.seed(32)

n <- 10000
p_d <- runif(n)
sens1 <- runif(n)
sens2 <- runif(n)
spec1 <- runif(n)
spec2 <- runif(n)

prior_mat <- cbind(p_d, sens1, sens2, spec1, spec2)

table_prior <- t(apply(prior_mat, 1, function(x) p_table(x[1], x[2:3], x[3:4])))

```


```{r}

par(mfrow = c(2, 2))

hist(table_prior[, 1], xlab = c("p00"))
hist(table_prior[, 2], xlab = c("p01"))
hist(table_prior[, 3], xlab = c("p10"))
hist(table_prior[, 4], xlab = c("p11"))

```

Average cell probablities are (`r round(colMeans(table_prior), digits = 2)`). Prior to seeing any data, the model expects counts to be evenly distributed across the four cells.

We now simulate a 2x2 table based on 500 pairs of test results according to the model specifications mentioned above, namely a disease prevalence of 0.2 and sensitivity and specificity of 0.95 and 0.99 for each of the two swabs. The observed data are

```{r}


two_by_two <- function(n_tables, n, p_disease, test_sens, test_spec){
  
  p_00 <- prod(p_disease, 1 - test_sens) + prod(1 - p_disease, test_spec)
  
  p_11 <- prod(p_disease, test_sens) + prod(1 - p_disease, 1 - test_spec)
  
  p_01 <- p_disease*test_sens[2]*(1 - test_sens[1]) + (1 - p_disease)*test_spec[1]*(1 - test_spec[2])
  
  p_10 <- 1 - (p_00 + p_11 + p_01)
  
  tables <- t(rmultinom(n = n_tables, size = n, prob = c(p_00, p_01, p_10, p_11)))
  
  if(n_tables == 1) tables <- as.vector(tables)
  
  tables
}

```

```{r}

set.seed(442)

example_dat <- two_by_two(1,500, p_disease = .2, test_sens = c(.95, .95), test_spec = c(.99, .99) )

example_mat <- matrix(example_dat, nrow = 2, ncol = 2)

rownames(example_mat) <- c("T_2 = 0", "T_2 = 1")
colnames(example_mat) <- c("T_1 = 0", "T_1 = 1")

```


```{r}

example_mat

```

We now analyze these data by drawing 8000 parameter values from the joint posterior distribution of all the model parameters. We then compute quantities of interest such as posterior means or intervals directly from these 8000 draws. 

```{r, cache = T, results = F, message = F, warning = F}

model_onesite <- stan_model("one_test.stan")

model_fit <- sampling(model_onesite, data = list(
  K = 4,
  y1 = example_dat,
  sens1shape = c(1, 1),
  sens2shape = c(1, 1),
  spec1shape = c(1, 1),
  spec2shape = c(1, 1),
  sens1limits = c(0, 1),
  sens2limits = c(0, 1),
  spec1limits = c(0, 1),
  spec2limits = c(0, 1),
  pd1shape = c(1, 1)
), iter = 4000, refresh = 0)


post_params <- rstan::extract(model_fit)
```

The following table shows estimated posterior means, standard deviations, and selected quantiles for the cell probabilities. 
```{r}

post_mat <- summary(model_fit)$summary[rownames(summary(model_fit)$summary) %in% paste0("p1[", 1:4, "]"), c(1, 3, 4:8) ]

rownames(post_mat) <- c("p00", "p01", "p10", "p11")
round(post_mat, digits = 2)


```


We can also visualize the shape of the posterior distribution by plotting the draws, similar to how we did for the prior distribution. As an example, we focus on the posterior distribution for $p_{00}$. The following figure shows the prior distribution for $p_{00}$ in blue and the posterior distribution for $p_{00}$ in red.

```{r}

plot(density(post_params$p1[,1]), xlab = "p00", col = 'red', xlim = c(0, 1), main = "")
lines(density(table_prior[, 1]), col = 'blue')

text(x = c(.25, .7), y = c(3, 10), labels = c( "Prior", "Posterior"), col = c('blue', 'red'))



```

We now "zoom in" on just the posterior distribution. The vertical lines show the true value of $p_{00}$ (the dashed vertical black line), the empirical estimate of $p_00$ (the solid black line), and the posterior mean of $p_{00}$ (the solid red line).

```{r}

plot(density(post_params$p1[,1]), xlab = "p00", col = 'red', main = "")

abline(v = p_table_example[1], lty = 2)
abline(v = mean(post_params$p1[, 1]), col = 'red')
abline(v = example_mat[1,1]/sum(example_mat))


```

We can see from this plot that the posterior mean tracks very closely with the empirical estimate.Bayesian and frequentist analyses tend to produce similar results when prior information is weak and the amount of data is large relative to the model's complexity (as it is in this case).

The posterior for other quantities of interest can be calculated from the posterior draws as well. For instance, since $P(Agree) = p_{00} + p_{11}$, we can simply add the draws for $p_00$ and $p_11$ to produce 8000 draws from the posterior distribution of $P(Agree)$. We plot the posterior distribution for $P(Agree)$ below.

```{r}

plot(density(post_params$p1[, 1] + post_params$p1[,4]), xlab = "P(Agree)", main = "")

```

We can then directly assess our degree of evidence for statements such $P(Agree) > 0.95$. Based on our model and the data we've observed, the posterior probability that $P(Agree)$ is greater than 0.95 is `r mean((post_params$p1[,1] + post_params$p1[,4]) > .95)`.

## Benefits of Bayesian Approach

- Ability to incorporate prior knowledge of disease prevalence and swab characteristics
- Ability to think in terms of a "generative" model that reflects what we know about how the test results arise
- Easily handles multilevel/hierarchical nature of the data e.g. variation in agreement measures across sites due to different underlying disease prevalences
- Flexibility with regards to sequential nature of data
- Direct assessment of uncertainty around questions of interest
- Straightforward joint inference for multiple outcomes



## Drawbacks of Bayesian Approach

- Need to specify prior
- Standard frequentist guarantees about error rates may not apply (can investigate via simulation)
- Pragmatic concern: regulatory approval
